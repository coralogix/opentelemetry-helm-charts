# Default values for opentelemetry-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  domain: ""
  version: ""
  defaultApplicationName: ""
  defaultSubsystemName: ""
  deploymentEnvironmentName: ""

nameOverride: ""
fullnameOverride: ""

# Enable the creation of a CRD, instead of a normal Helm release.
# If enabled, the OpenTelemetryCollector CRD will be generated
# instead of the collector manifests. It is recommended to use this
# in conjunction with 'helm template' and 'kubectl apply', if you
# wish to run the collector with the OpenTelemetry operator.
# Most of the values below correspond to the CRD spec, for full documentation see
# https://github.com/open-telemetry/opentelemetry-operator/blob/main/docs/api.md#opentelemetrycollector
collectorCRD:
  generate: false
  updateStrategy: ""
  resources: {}

# Valid values are "daemonset", "deployment", and "statefulset".
mode: "daemonset"

# If the target allocator is enabled, the service account, cluster role
# and cluster role binding will be generated as well, with appropriate
# permissions, necessary for target allocator to work. If you do not
# wish to create these objects and would like to specify your own
# service account, provide it below.
targetAllocator:
  enabled: false
  replicas: 1
  tolerations: []
  affinity: {}
  scrapeConfigs: []
  prometheusCR:
    serviceMonitorNamespaceSelector: {}
    podMonitorNamespaceSelector: {}
    enabled: false
    podMonitorSelector: {}
    serviceMonitorSelector: {}
    scrapeInterval: "30s"
  allocationStrategy: ""
  # Target allocator image defaults to repository and tag set by
  # the operator (this should correspond to operator's version).
  # Set appropriate values to override.
  image:
    repository: ""
    pullPolicy: IfNotPresent
    tag: ""
    digest: ""
  # Name of the service account to be used by the target allocator.
  # If set, no service account or cluster role for target allocator
  # will be created.
  serviceAccount: ""
  resources: {}
  volumeMounts: []
  volumes: []

  serviceMonitor:
    enabled: false
    extraLabels: {}

  pdb:
    create: false
    maxUnavailable: 25%

  livenessProbe:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  minReadySeconds: 60

# When using windows set this to true and make sure
# to use collector image built for windows.
isWindows: false
# When using GKE Autopilot set this to "gke/autopilot"
distribution: ""

# Specify which namespace should be used to deploy the resources into
namespaceOverride: ""

# Extra config for the collector
# This is used to add extra receivers, processors, exporters, and connectors
# to the "logs", "metrics", and "traces" pipelines and other pipelines managed by the chart.
# Processors are added at the end of the pipeline before batching.
# You cannot use this to add new pipelines, instead use the
# config.service.pipelines field to add new pipelines.
extraConfig: {}
# extraConfig:
#   receivers:
#     otlp/3:
#       protocols:
#         grpc:
#           endpoint: ${env:MY_POD_IP}:4317
#         http:
#           endpoint: ${env:MY_POD_IP}:4318
#   processors:
#     batch: {}
#   exporters:
#     debug/2: {}
#   connectors:
#     spanmetrics/2:
#       metrics:
#         enabled: true
#   service:
#     pipelines:
#       logs:
#         receivers:
#         - otlp/3
#         processors:
#         - batch/2
#         exporters:
#         - debug/2
#       metrics:
#         receivers:
#         - otlp/3
#         - spanmetrics/2
# Handles basic configuration of components that
# also require k8s modifications to work correctly.
# .Values.config can be used to modify/add to a preset
# component configuration, but CANNOT be used to remove
# preset configuration. If you require removal of any
# sections of a preset configuration, you cannot use
# the preset. Instead, configure the component manually in
# .Values.config and use the other fields supplied in the
# values.yaml to configure k8s as necessary.
presets:
  # Configures the collector to collect logs.
  # Adds the filelog receiver to the logs pipeline
  # and adds the necessary volumes and volume mounts.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#filelog-receiver for details on the receiver.
  logsCollection:
    enabled: false
    includeCollectorLogs: false
    # Time since last read of data from file, after which currently buffered log should be send to pipeline. A value of 0 will disable forced flushing.
    forceFlushPeriod: "0"
    # Enabling this writes checkpoints in /var/lib/otelcol/ host directory.
    # Note this changes collector's user to root, so that it can write to host directory.
    storeCheckpoints: false
    # The maximum bytes size of the recombined field.
    # Once the size exceeds the limit, all received entries of the source will be combined and flushed.
    maxRecombineLogSize: 1048576
    # The maximum number of consecutive entries that will be combined into a single entry before the match occurs
    maxUnmatchedBatchSize: 1
    # The maximum number of consecutive entries that will be combined into a single entry.
    maxBatchSize: 1000
    # Add additional filelog operators
    # ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/stanza/docs/operators
    extraFilelogOperators: []
    # - type: recombine
    #   combine_field: body
    #   source_identifier: attributes["log.file.path"]
    #   is_first_entry: body matches "^([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}|[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})"
    # Configure specific multline options for namespaces / pods / container names.
    multilineConfigs: []
    #   - namespaceName:
    #       value: kube-system
    #     podName:
    #       value: app-.*
    #       useRegex: true
    #     containerName:
    #       value: http
    #     firstEntryRegex: ^[^\s].*
    #     combineWith: ""

    # Configures the collector to reduce log attributes by removing specified log record attributes.
    # This sub-preset adds a transform processor to the logs pipeline.
    reduceLogAttributes:
      enabled: false
      denylist:
        - "log.iostream"
        - "log.file.path"
        - "logtag"
        - "time"

  # Configures the collector to collect profiles.
  profilesCollection:
    enabled: false
  # Configures the collector to collect host metrics.
  # Adds the hostmetrics receiver to the metrics pipeline
  # and adds the necessary volumes and volume mounts.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#host-metrics-receiver for details on the receiver.
  hostMetrics:
    enabled: false
    # Enables process scraper for hostMetrics
    # This needs privileged security context
    process:
      enabled: false
    # collectionInterval: 30s
  # Configures the Kubernetes Processor to add Kubernetes metadata.
  # Adds the k8sattributes processor to all the pipelines
  # and adds the necessary rules to ClusteRole.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-attributes-processor for details on the receiver.
  kubernetesAttributes:
    enabled: false
    # When enabled the processor will extra all labels for an associated pod and add them as resource attributes.
    # The label's exact name will be the key.
    extractAllPodLabels: false
    # When enabled the processor will extra all annotations for an associated pod and add them as resource attributes.
    # The annotation's exact name will be the key.
    extractAllPodAnnotations: false
    # Specifies whether to extract pod UID into k8s.pod.uid attribute.
    podUid:
      enabled: true
    # Specifies whether to extract pod start time into k8s.pod.start_time attribute.
    podStartTime:
      enabled: true
    # When enabled adds node_from_env_var filter to limit processing to the node
    # specified by the K8S_NODE_NAME environment variable.
    nodeFilter:
      enabled: false
  # Configures the collector to collect node, pod, and container metrics from the API server on a kubelet..
  # Adds the kubeletstats receiver to the metrics pipeline
  # and adds the necessary rules to ClusteRole.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubeletstats-receiver for details on the receiver.
  kubeletMetrics:
    enabled: false
    # collectionInterval: 30s
  # Configures the collector to collect kubernetes events.
  # Adds the k8sobject receiver to the logs pipeline
  # and collects kubernetes events by default.
  # Best used with mode = deployment or statefulset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-objects-receiver for details on the receiver.
  kubernetesEvents:
    enabled: false
    clusterName: ""
  # Configures the Kubernetes Cluster Receiver to collect cluster-level metrics.
  # Adds the k8s_cluster receiver to the metrics pipeline
  # and adds the necessary rules to ClusteRole.
  # Best used with mode = deployment or statefulset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-cluster-receiver for details on the receiver.
  clusterMetrics:
    enabled: false
    # collectionInterval: 30s
    customMetrics:
      enabled: false

  # Configures the collector to collect extra Kubernetes metrics, such as pod status and Kubernetes API version.
  # Best used with mode = deployment or statefulset
  kubernetesExtraMetrics:
    enabled: false
    # When false (default), collects only fs and cfs throttling metrics. When true, collect all metrics.
    scrapeAll: false

  # Configures the collector to add k8s.cluster.name and cx.otel_integration.name
  # to metrics
  metadata:
    enabled: false
    clusterName: ""
    integrationName: ""

  # Configures the collector to export span metrics.
  spanMetrics:
    enabled: false
    # Configures collector to add service.version to spanMetrics dimensions
    serviceVersion:
      enabled: true
    dbMetrics:
      enabled: true
      # Configures collector to add service.version to dbMetrics dimensions
      serviceVersion:
        enabled: true
      extraDimensions: []
    # Configures the collector to collect and export error information from spans.
    errorTracking:
      enabled: true
    extraDimensions:
      - name: http.method
      - name: cgx.transaction
      - name: cgx.transaction.root
      - name: status_code
      - name: db.namespace
      - name: db.operation.name
      - name: db.collection.name
      - name: db.system
    histogramBuckets:
      [1ms, 4ms, 10ms, 20ms, 50ms, 100ms, 200ms, 500ms, 1s, 2s, 5s]

    # collectionInterval: 30s
    # metricsExpiration: 5m
    # aggregationCardinalityLimit sets the maximum amount of aggregation keys
    # kept in memory. 0 means unlimited.
    aggregationCardinalityLimit: 0
    # spanNameReplacePattern configures a regex pattern to replace the span name. This is useful to reduce cardinality
    # in cases where span names are too specific (e.g. they include UUIDs or other unique identifiers). If the regex pattern
    # matches the span name, the replacement will be used instead. The parameter accepts a list, i.e. mutliple replace patterns
    # can be specified. For more information and examples, see:
    # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#replace_pattern
    spanNameReplacePattern: []
    # - regex: ""
    #   replacement: ""
  # Configures the collector to export span metrics with different histogram bucket options
  # for different applications. Applications are selected and routed to different pipelines
  # using OTTL. For more information see https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/connector/routingconnector
  spanMetricsMulti:
    enabled: false
    extraDimensions:
      - name: http.method
      - name: cgx.transaction
      - name: cgx.transaction.root
      - name: status_code
      - name: db.namespace
      - name: db.operation.name
      - name: db.collection.name
      - name: db.system
    defaultHistogramBuckets:
      [1ms, 4ms, 10ms, 20ms, 50ms, 100ms, 200ms, 500ms, 1s, 2s, 5s]
    # collectionInterval: 30s
    # metricsExpiration: 5m
    aggregationCardinalityLimit: 0
    configs:
      []
      #  - selector: route() where attributes["service.name"] == "one"
      #    histogramBuckets: []

  # Configures the collector to use loadbalancing exporter.
  loadBalancing:
    enabled: false
    # traceID, service, resource or metric
    # See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/loadbalancingexporter
    routingKey: "traceID"
    # DNS hostname that will resolve to all IP addresses of the target.
    hostname: ""

  # Configures the collector to export data to Coralogix.
  coralogixExporter:
    enabled: false
    # Use pipelines (plural) for multiple pipeline selection.
    # When pipelines is set, it takes precedence over pipeline.
    # Possible values: "all", "metrics", "traces", "logs" and "none".
    # Examples:
    #   pipelines: ["metrics", "traces"]       # Both metrics and traces
    #   pipelines: ["all"]                     # All pipelines
    #   pipelines: ["none"]                    # No pipelines
    pipelines: ["all"]
    # DEPRECATED: Please use `pipelines` instead.
    # pipeline: "all"
    privateKey: ${env:CORALOGIX_PRIVATE_KEY}
    domain: ""
    version: ""
    defaultApplicationName: ""
    defaultSubsystemName: ""

  # Configures grouping spans into transactions and enabling Coralogix transaction processor.
  transactions:
    enabled: false
    waitDuration: 30s

  # Removes a subset of attributes to be sent to Coralogix. It uses a denylist policy.
  # The possible elements that can be added in the `pipelines` list are:
  # - "all" - applied to all pipelines
  # - "metrics" - applied to the metrics pipeline
  # - "traces" - applied to the traces pipeline
  # - "logs" - applied to the logs pipeline
  reduceResourceAttributes:
    enabled: false
    pipelines: ["all"]
    denylist:
      metrics:
        - azure.resourcegroup.name
        - azure.vm.name
        - azure.vm.scaleset.name
        - azure.vm.size
        - cloud.account.id
        - cloud.availability_zone
        - cloud.platform
        - cloud.provider
        - cloud.region
        - container.id
        - faas.id
        - faas.instance
        - faas.name
        - faas.version
        - gcp.cloud_run.job.execution
        - gcp.cloud_run.job.task_index
        - gcp.gce.instance_group_manager.name
        - gcp.gce.instance_group_manager.region
        - gcp.gce.instance_group_manager.zone
        - host.image.id
        - host.type
        - k8s.cronjob.uid
        - k8s.daemonset.uid
        - k8s.deployment.uid
        - k8s.hpa.uid
        - k8s.job.uid
        - k8s.namespace.uid
        - k8s.node.uid
        - k8s.pod.start_time
        - k8s.pod.uid
        - k8s.replicaset.uid
        - k8s.statefulset.uid
        - os.type
        - net.host.name
        - net.host.port
      traces:
        - azure.resourcegroup.name
        - azure.vm.name
        - azure.vm.scaleset.name
        - azure.vm.size
        - cloud.account.id
        - cloud.availability_zone
        - cloud.platform
        - cloud.provider
        - cloud.region
        - container.id
        - faas.id
        - faas.instance
        - faas.name
        - faas.version
        - gcp.cloud_run.job.execution
        - gcp.cloud_run.job.task_index
        - gcp.gce.instance_group_manager.name
        - gcp.gce.instance_group_manager.region
        - gcp.gce.instance_group_manager.zone
        - host.image.id
        - host.type
        - k8s.cronjob.uid
        - k8s.daemonset.uid
        - k8s.deployment.uid
        - k8s.hpa.uid
        - k8s.job.uid
        - k8s.namespace.uid
        - k8s.node.uid
        - k8s.pod.start_time
        - k8s.pod.uid
        - k8s.replicaset.uid
        - k8s.statefulset.uid
        - os.type
      logs:
        - azure.resourcegroup.name
        - azure.vm.name
        - azure.vm.scaleset.name
        - azure.vm.size
        - cloud.account.id
        - cloud.availability_zone
        - cloud.platform
        - cloud.provider
        - cloud.region
        - container.id
        - faas.id
        - faas.instance
        - faas.name
        - faas.version
        - gcp.cloud_run.job.execution
        - gcp.cloud_run.job.task_index
        - gcp.gce.instance_group_manager.name
        - gcp.gce.instance_group_manager.region
        - gcp.gce.instance_group_manager.zone
        - host.image.id
        - host.type
        - k8s.cronjob.uid
        - k8s.daemonset.uid
        - k8s.deployment.uid
        - k8s.hpa.uid
        - k8s.job.uid
        - k8s.namespace.uid
        - k8s.node.uid
        - k8s.pod.start_time
        - k8s.pod.uid
        - k8s.replicaset.uid
        - k8s.statefulset.uid
        - os.type
  # Applies semantic convention transformations.
  semconv:
    enabled: false
  # Configures Kubernetes Object receiver to collect Kubernetes Resources.
  kubernetesResources:
    enabled: false
    filterWorkflows:
      enabled: true
    filterStatements: []
    transformStatements: []
    dropManagedFields:
      enabled: false
    periodicCollection:
      enabled: true
  # Configures Host Metrics receiver to collect Entity Events.
  hostEntityEvents:
    enabled: false
  # Configures the collector to communicate with the remote Coralogix fleet management service.
  fleetManagement:
    enabled: false
    # IMPORTANT: The `supervisor` preset is in alpha stage and under heavy development.
    # It's not recommended for use in production environments.
    # Support will be provided only when it reaches a stable release.
    supervisor:
      enabled: false
    agentType: ""
    clusterName: ""
    integrationID: ""

  # Configures service telemetry resource with Kubernetes metadata
  k8sResourceAttributes:
    enabled: false
    agentType: ""

  # Configures the collector to collect MySQL metrics .
  mysql:
    # metrics enables collection of MySQL metrics.
    metrics:
      enabled: false
      # If you have multiple instance you'd like to collect, specify each instance in the list below.
      # If you have only one instance, you don't need to specify the labelSelectors.
      instances:
        - username: "root"
          password: ${env:MYSQL_PASSWORD}
          port: 3306
          # collectionInterval: 30s
          # labelSelectors:
          #   app.kubernetes.io/name: "mysql"
          #   app.kubernetes.io/instance: "mysql-logs"
    # extraLogs enables collection of MySQL log files that are not out put to stdin / stderr. These include
    # for example general or slow query logs.
    # This configuration option is valid ONLY with the OpenTelemetry Operator, when collectorCRD.generate = true.
    # Otherwise enabling extra logs has no effect.
    extraLogs:
      enabled: false
      # volumeMountName specifies the name of the volume mount. It should correspond to the volume name of the MySQL data volume.
      volumeMountName: "data"
      # mountPath specifies the path at which to mount the volume. This should correspond the mount path of your MySQL data volume.
      # Provide this parameter without trailing slash.
      mountPath: "/var/lib/mysql"
      # logFilesPath specifies which directory Rsto watch for log files. This will typically be the MySQL data directory,
      # such as /var/lib/mysql. If not specified, the value of mountPath will be used.
      logFilesPath: ""
      # logFilesExtension specifies the file extension of the log files to watch. This will normally be .log.
      logFilesExtension: ".log"

  # Configures probabilistic head sampling for traces
  headSampling:
    enabled: false
    # Percentage of traces to sample (0-100)
    percentage: 10
    # Sampling mode - "proportional", "equalizing", "hash_seed"
    mode: "proportional"

  # Configures the collector to collect its own metrics using Prometheus receiver.
  # Adds the prometheus receiver to the metrics pipeline with a scrape config
  # targeting the collector's metrics endpoint.
  # Also adds a transform processor to clean up metric names and attributes.
  collectorMetrics:
    enabled: true
    # Pipeline to add the collector metrics receiver and processor to. Possible values are
    # "metrics" and "none".
    pipeline: metrics
    # Scrape interval for collector metrics
    scrapeInterval: "30s"

  # Configures the collector to receive Jaeger data in all supported protocols.
  # Adds the jaeger receiver to the traces pipeline with all protocols configured.
  jaegerReceiver:
    enabled: false

  # Configures the collector to receive Zipkin data.
  # Adds the zipkin receiver to the traces pipeline.
  zipkinReceiver:
    enabled: false

  # Configures the collector to receive OTLP data.
  # Adds the OTLP receiver to the traces, metrics, and logs pipelines.
  otlpReceiver:
    enabled: true
    # Default otlp grpc server message size limit is 4mib, which might be too low.
    maxRecvMsgSizeMiB: 20

  # Configures the collector to receive StatsD metrics.
  # Adds the statsd receiver to the metrics pipeline.
  statsdReceiver:
    enabled: false

  # Configures the collector to expose zPages for debugging.
  zpages:
    enabled: false
    endpoint: localhost:55679

  # Configures the collector to expose pprof for profiling.
  pprof:
    enabled: false
    endpoint: localhost:1777

  # Configuration for the batch processor.
  batch:
    enabled: false
    sendBatchSize: 1024
    sendBatchMaxSize: 2048
    timeout: "1s"

  # Configures resource detection processors to add system and environment information.
  # Also configures volumes and volume mounts for the collector.
  resourceDetection:
    enabled: false
    # Pipeline to add the resource detection processors to. Possible values are
    # "all", "metrics", "traces" and "none".
    pipeline: all
    # Specifies whether to add the node name to OTEL_RESOURCE_ATTRIBUTES.
    k8sNodeName:
      enabled: true
    # Specifies whether to mount /var/lib/dbus/machine-id from the host.
    dbusMachineId:
      enabled: true
    # Allows setting the deployment environment name in OTEL_RESOURCE_ATTRIBUTES.
    deploymentEnvironmentName: ""

configMap:
  # Specifies whether a configMap should be created (true by default)
  create: true

# Base collector configuration.
# Supports templating. To escape existing instances of {{ }}, use {{` <original content> `}}.
# For example, {{ REDACTED_EMAIL }} becomes {{` {{ REDACTED_EMAIL }} `}}.
config:
  exporters:
    debug: {}
  extensions:
    # The health_check extension is mandatory for this chart.
    # Without the health_check extension the collector will fail the readiness and liveliness probes.
    # The health_check extension can be modified, but should never be removed.
    health_check:
      endpoint: ${env:MY_POD_IP}:13133
    memory_ballast: {}
  receivers: {}
  processors:
    # If set to null, will be overridden with values based on k8s resource limits
    memory_limiter: null
  service:
    telemetry:
      logs:
        encoding: json
    extensions:
      - health_check
    pipelines:
      logs:
        exporters:
          - debug
        processors:
          - memory_limiter
        receivers: []
      metrics:
        exporters:
          - debug
        processors:
          - memory_limiter
        receivers: []
      traces:
        exporters:
          - debug
        processors:
          - memory_limiter
        receivers: []

image:
  # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
  repository: ""
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
  # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
  digest: ""
imagePullSecrets: []

# OpenTelemetry Collector executable
command:
  name: ""
  extraArgs: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

clusterRole:
  # Specifies whether a clusterRole should be created
  # Some presets also trigger the creation of a cluster role and cluster role binding.
  # If using one of those presets, this field is no-op.
  create: false
  # Annotations to add to the clusterRole
  # Can be used in combination with presets that create a cluster role.
  annotations: {}
  # The name of the clusterRole to use.
  # If not set a name is generated using the fullname template
  # Can be used in combination with presets that create a cluster role.
  name: ""
  # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  # Can be used in combination with presets that create a cluster role to add additional rules.
  rules: []
  # - apiGroups:
  #   - ''
  #   resources:
  #   - 'pods'
  #   - 'nodes'
  #   verbs:
  #   - 'get'
  #   - 'list'
  #   - 'watch'

  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    # Can be used in combination with presets that create a cluster role binding.
    annotations: {}
    # The name of the clusterRoleBinding to use.
    # If not set a name is generated using the fullname template
    # Can be used in combination with presets that create a cluster role binding.
    name: ""

podSecurityContext: {}
securityContext: {}

nodeSelector: {}
tolerations: []
affinity: {}
topologySpreadConstraints: []

# DEPRECATED: Please use `priorityClass` instead.
# Allows for pod scheduler prioritisation
priorityClassName: ""

priorityClass:
  # Specifies whether a priorityClass should be created.
  create: false
  # The name of the clusterRole to use.
  # If not set a name is generated using the fullname template.
  name: ""
  # Sets the priority value of the priority class.
  priorityValue: 1000000000

extraEnvs: []
extraEnvsFrom: []
extraVolumes: []
extraVolumeMounts: []

# Configuration for ports
# nodePort is also allowed
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
    protocol: TCP
    # nodePort: 30317
    appProtocol: grpc
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    hostPort: 4318
    protocol: TCP

  metrics:
    # The metrics port is disabled by default. However you need to enable the port
    # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
    enabled: false
    containerPort: 8888
    servicePort: 8888
    protocol: TCP

# Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
resources: {}
# resources:
#   limits:
#     cpu: 250m
#     memory: 512Mi

podAnnotations: {}

podLabels: {}

# Common labels to add to all otel-collector resources. Evaluated as a template.
additionalLabels: {}
#  app.kubernetes.io/part-of: my-app

# Host networking requested for this pod. Use the host's network namespace.
hostNetwork: false

# Network mode for collector endpoints. Set to `ipv6` when running in IPv6
# clusters so that endpoint IP addresses are wrapped in brackets.
networkMode: ipv4

# Pod DNS policy ClusterFirst, ClusterFirstWithHostNet, None, Default, None
dnsPolicy: ""

# Custom DNS config. Required when DNS policy is None.
dnsConfig: {}

# only used with deployment mode
replicaCount: 1

# only used with deployment mode
revisionHistoryLimit: 10

annotations: {}

# List of extra sidecars to add
extraContainers: []
# extraContainers:
#   - name: test
#     command:
#       - cp
#     args:
#       - /bin/sleep
#       - /test/sleep
#     image: busybox:latest
#     volumeMounts:
#       - name: test
#         mountPath: /test

# List of init container specs, e.g. for copying a binary to be executed as a lifecycle hook.
# Another usage of init containers is e.g. initializing filesystem permissions to the OTLP Collector user `10001` in case you are using persistence and the volume is producing a permission denied error for the OTLP Collector container.
initContainers: []
# initContainers:
#   - name: test
#     image: busybox:latest
#     command:
#       - cp
#     args:
#       - /bin/sleep
#     args:
#       - /test/sleep
#     volumeMounts:
#       - name: test
#         mountPath: /test
#  - name: init-fs
#    image: busybox:latest
#    command:
#      - sh
#      - '-c'
#      - 'chown -R 10001: /var/lib/storage/otc' # use the path given as per `extensions.file_storage.directory` & `extraVolumeMounts[x].mountPath`
#    volumeMounts:
#      - name: opentelemetry-collector-data # use the name of the volume used for persistence
#        mountPath: /var/lib/storage/otc # use the path given as per `extensions.file_storage.directory` & `extraVolumeMounts[x].mountPath`

# Pod lifecycle policies.
lifecycleHooks: {}
# lifecycleHooks:
#   preStop:
#     exec:
#       command:
#       - /test/sleep
#       - "5"

# startup probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
startupProbe:
  {}
  # Number of seconds after the container has started before startup probes are initiated.
  # initialDelaySeconds: 1
  # How often in seconds to perform the probe.
  # periodSeconds: 10
  # Number of seconds after which the probe times out.
  # timeoutSeconds: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  # Duration in seconds the pod needs to terminate gracefully upon probe failure.
  # terminationGracePeriodSeconds: 10
  # httpGet:
  #   port: 13133
  #   path: /

# liveness probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
livenessProbe:
  # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
  # initialDelaySeconds: 1
  # How often in seconds to perform the probe.
  # periodSeconds: 10
  # Number of seconds after which the probe times out.
  # timeoutSeconds: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  # Duration in seconds the pod needs to terminate gracefully upon probe failure.
  # terminationGracePeriodSeconds: 10
  httpGet:
    port: 13133
    path: /

# readiness probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
readinessProbe:
  # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
  # initialDelaySeconds: 1
  # How often (in seconds) to perform the probe.
  # periodSeconds: 10
  # Number of seconds after which the probe times out.
  # timeoutSeconds: 1
  # Minimum consecutive successes for the probe to be considered successful after having failed.
  # successThreshold: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  httpGet:
    port: 13133
    path: /

service:
  # Enable the creation of a Service.
  # By default, it's enabled on mode != daemonset.
  # However, to enable it on mode = daemonset, its creation must be explicitly enabled
  # enabled: true

  type: ClusterIP
  # type: LoadBalancer
  # loadBalancerIP: 1.2.3.4
  # loadBalancerSourceRanges: []

  # By default, Service of type 'LoadBalancer' will be created setting 'externalTrafficPolicy: Cluster'
  # unless other value is explicitly set.
  # Possible values are Cluster or Local (https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)
  # externalTrafficPolicy: Cluster

  annotations: {}

  # By default, Service will be created setting 'internalTrafficPolicy: Local' on mode = daemonset
  # unless other value is explicitly set.
  # Setting 'internalTrafficPolicy: Cluster' on a daemonset is not recommended
  # internalTrafficPolicy: Cluster

ingress:
  enabled: false
  # annotations: {}
  # ingressClassName: nginx
  # hosts:
  #   - host: collector.example.com
  #     paths:
  #       - path: /
  #         pathType: Prefix
  #         port: 4318
  # tls:
  #   - secretName: collector-tls
  #     hosts:
  #       - collector.example.com

  # Additional ingresses - only created if ingress.enabled is true
  # Useful for when differently annotated ingress services are required
  # Each additional ingress needs key "name" set to something unique
  additionalIngresses: []
  # - name: cloudwatch
  #   ingressClassName: nginx
  #   annotations: {}
  #   hosts:
  #     - host: collector.example.com
  #       paths:
  #         - path: /
  #           pathType: Prefix
  #           port: 4318
  #   tls:
  #     - secretName: collector-tls
  #       hosts:
  #         - collector.example.com

podMonitor:
  # The pod monitor by default scrapes the metrics port.
  # The metrics port needs to be enabled as well.
  enabled: false
  metricsEndpoints:
    - port: metrics
  # interval: 15s
  targetLabels: []

  # additional labels for the PodMonitor
  extraLabels: {}
  #   release: kube-prometheus-stack

serviceMonitor:
  # The service monitor by default scrapes the metrics port.
  # The metrics port needs to be enabled as well.
  enabled: false
  metricsEndpoints:
    - port: metrics
      # interval: 15s
  targetLabels: []

  # additional labels for the ServiceMonitor
  extraLabels: {}
  #  release: kube-prometheus-stack

# PodDisruptionBudget is used only if deployment enabled
podDisruptionBudget:
  enabled: false
#   minAvailable: 2
#   maxUnavailable: 1

# autoscaling is used only if mode is "deployment" or "statefulset"
autoscaling:
  enabled: false
  # hpa or custom if you are using a custom autoscaler, such as KEDA
  mode: "hpa"
  minReplicas: 1
  maxReplicas: 10
  behavior: {}
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

rollout:
  rollingUpdate: {}
  # When 'mode: daemonset', maxSurge cannot be used when hostPort is set for any of the ports
  # maxSurge: 25%
  # maxUnavailable: 0
  strategy: RollingUpdate

prometheusRule:
  enabled: false
  groups: []
  # Create default rules for monitoring the collector
  defaultRules:
    enabled: false

  # additional labels for the PrometheusRule
  extraLabels: {}

statefulset:
  # volumeClaimTemplates for a statefulset
  volumeClaimTemplates: []
  podManagementPolicy: "Parallel"

networkPolicy:
  enabled: false

  # Annotations to add to the NetworkPolicy
  annotations: {}

  # Configure the 'from' clause of the NetworkPolicy.
  # By default this will restrict traffic to ports enabled for the Collector. If
  # you wish to further restrict traffic to other hosts or specific namespaces,
  # see the standard NetworkPolicy 'spec.ingress.from' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  allowIngressFrom: []
  # # Allow traffic from any pod in any namespace, but not external hosts
  # - namespaceSelector: {}
  # # Allow external access from a specific cidr block
  # - ipBlock:
  #     cidr: 192.168.1.64/32
  # # Allow access from pods in specific namespaces
  # - namespaceSelector:
  #     matchExpressions:
  #       - key: kubernetes.io/metadata.name
  #         operator: In
  #         values:
  #           - "cats"
  #           - "dogs"

  # Add additional ingress rules to specific ports
  # Useful to allow external hosts/services to access specific ports
  # An example is allowing an external prometheus server to scrape metrics
  #
  # See the standard NetworkPolicy 'spec.ingress' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  extraIngressRules: []
  # - ports:
  #   - port: metrics
  #     protocol: TCP
  #   from:
  #     - ipBlock:
  #         cidr: 192.168.1.64/32

  # Restrict egress traffic from the OpenTelemetry collector pod
  # See the standard NetworkPolicy 'spec.egress' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  egressRules: []
  #  - to:
  #      - namespaceSelector: {}
  #      - ipBlock:
  #          cidr: 192.168.10.10/24
  #    ports:
  #      - port: 1234
  #        protocol: TCP

# When enabled, the chart will set the GOMEMLIMIT env var to 80% of the configured
# resources.limits.memory and remove the memory ballast extension.
# If no resources.limits.memory are defined enabling does nothing.
# In a future release this setting will be enabled by default.
# See https://github.com/open-telemetry/opentelemetry-helm-charts/issues/891
# for more details.
useGOMEMLIMIT: true
