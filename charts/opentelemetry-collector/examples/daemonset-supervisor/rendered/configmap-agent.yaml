---
# Source: opentelemetry-collector/templates/configmap-agent.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-opentelemetry-collector-agent
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-collector-0.117.4
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: example
    app.kubernetes.io/version: "0.129.1"
    app.kubernetes.io/managed-by: Helm
    
data:
  relay: |
    connectors:
      forward/db: {}
      forward/sampled: {}
      spanmetrics:
        aggregation_cardinality_limit: 0
        dimensions:
        - name: http.method
        - name: cgx.transaction
        - name: cgx.transaction.root
        - name: status_code
        - name: db.namespace
        - name: db.operation.name
        - name: db.collection.name
        - name: db.system
        - name: http.response.status_code
        - name: rpc.grpc.status_code
        - name: service.version
        histogram:
          explicit:
            buckets:
            - 1ms
            - 1s
            - 10s
        metrics_expiration: 5m
        metrics_flush_interval: 30s
        namespace: ""
      spanmetrics/db:
        aggregation_cardinality_limit: 0
        dimensions:
        - name: db.namespace
        - name: db.operation.name
        - name: db.collection.name
        - name: db.system
        - name: service.version
        - name: cgx.transaction
        - name: cgx.transaction.root
        histogram:
          explicit:
            buckets:
            - 100us
            - 1ms
            - 2ms
            - 2.5ms
            - 4ms
            - 6ms
            - 10ms
            - 100ms
            - 250ms
        metrics_expiration: 5m
        metrics_flush_interval: 30s
        namespace: db
    exporters:
      coralogix:
        application_name: otel
        application_name_attributes:
        - k8s.namespace.name
        - service.namespace
        domain: eu2.coralogix.com
        logs:
          headers:
            X-Coralogix-Distribution: helm-otel-integration/1.0
        metrics:
          headers:
            X-Coralogix-Distribution: helm-otel-integration/1.0
        private_key: ${env:CORALOGIX_PRIVATE_KEY}
        subsystem_name: integration
        subsystem_name_attributes:
        - k8s.deployment.name
        - k8s.statefulset.name
        - k8s.daemonset.name
        - k8s.cronjob.name
        - service.name
        timeout: 30s
        traces:
          headers:
            X-Coralogix-Distribution: helm-otel-integration/1.0
      debug: {}
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
      k8s_observer:
        auth_type: serviceAccount
        observe_pods: true
    processors:
      batch:
        send_batch_max_size: 2048
        send_batch_size: 1024
        timeout: 1s
      filter/db_spanmetrics:
        traces:
          span:
          - attributes["db.system"] == nil
      filter/k8s_extra_metrics:
        metrics:
          metric:
          - resource.attributes["service.name"] == "kubernetes-apiserver" and name !=
            "kubernetes_build_info"
          - resource.attributes["service.name"] == "kubernetes-cadvisor" and (name !=
            "container_fs_writes_total" and name != "container_fs_reads_total" and name
            != "container_fs_writes_bytes_total" and name != "container_fs_reads_bytes_total"
            and name != "container_fs_usage_bytes" and name != "container_cpu_cfs_throttled_periods_total"
            and name != "container_cpu_cfs_periods_total")
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.replicaset.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
        - sources:
          - from: resource_attribute
            name: k8s.job.name
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
      probabilistic_sampler:
        mode: proportional
        sampling_percentage: 10
      resourcedetection/env:
        detectors:
        - system
        - env
        override: false
        system:
          resource_attributes:
            host.id:
              enabled: true
        timeout: 2s
      resourcedetection/region:
        detectors:
        - gcp
        - ec2
        - azure
        override: true
        timeout: 2s
      transform/db:
        error_mode: silent
        trace_statements:
        - context: span
          statements:
          - set(attributes["db.namespace"], attributes["db.name"]) where attributes["db.namespace"]
            == nil
          - set(attributes["db.namespace"], attributes["server.address"]) where attributes["db.namespace"]
            == nil
          - set(attributes["db.namespace"], attributes["network.peer.name"]) where attributes["db.namespace"]
            == nil
          - set(attributes["db.namespace"], attributes["net.peer.name"]) where attributes["db.namespace"]
            == nil
          - set(attributes["db.namespace"], attributes["db.system"]) where attributes["db.namespace"]
            == nil
          - set(attributes["db.operation.name"], attributes["db.operation"]) where attributes["db.operation.name"]
            == nil
          - set(attributes["db.collection_name"], attributes["db.sql.table"]) where attributes["db.collection_name"]
            == nil
          - set(attributes["db.collection_name"], attributes["db.cassandra.table"]) where
            attributes["db.collection.name"] == nil
          - set(attributes["db.collection_name"], attributes["db.mongodb.collection"])
            where attributes["db.collection.name"] == nil
          - set(attributes["db.collection_name"], attributes["db.redis.database_index"])
            where attributes["db.collection.name"] == nil
          - set(attributes["db.collection_name"], attributes["db.elasticsearch.path_parts.index"])
            where attributes["db.collection.name"] == nil
          - set(attributes["db.collection_name"], attributes["db.cosmosdb.container"])
            where attributes["db.collection.name"] == nil
          - set(attributes["db.collection_name"], attributes["aws_dynamodb.table_names"])
            where attributes["db.collection.name"] == nil
      transform/k8s_attributes:
        log_statements:
        - context: resource
          statements:
          - set(attributes["k8s.deployment.name"], attributes["k8s.replicaset.name"])
          - replace_pattern(attributes["k8s.deployment.name"], "^(.*)-[0-9a-zA-Z]+$",
            "$$1") where attributes["k8s.replicaset.name"] != nil
          - delete_key(attributes, "k8s.replicaset.name")
        metric_statements:
        - context: resource
          statements:
          - set(attributes["k8s.deployment.name"], attributes["k8s.replicaset.name"])
          - replace_pattern(attributes["k8s.deployment.name"], "^(.*)-[0-9a-zA-Z]+$",
            "$$1") where attributes["k8s.replicaset.name"] != nil
          - delete_key(attributes, "k8s.replicaset.name")
        trace_statements:
        - context: resource
          statements:
          - set(attributes["k8s.deployment.name"], attributes["k8s.replicaset.name"])
          - replace_pattern(attributes["k8s.deployment.name"], "^(.*)-[0-9a-zA-Z]+$",
            "$$1") where attributes["k8s.replicaset.name"] != nil
          - delete_key(attributes, "k8s.replicaset.name")
      transform/prometheus:
        error_mode: ignore
        metric_statements:
        - context: metric
          statements:
          - replace_pattern(name, "_total$", "") where resource.attributes["service.name"]
            == "opentelemetry-collector"
        - context: resource
          statements:
          - set(attributes["k8s.pod.ip"], attributes["net.host.name"]) where attributes["service.name"]
            == "opentelemetry-collector"
          - delete_key(attributes, "service_name") where attributes["service.name"] ==
            "opentelemetry-collector"
        - context: datapoint
          statements:
          - delete_key(attributes, "service_name") where resource.attributes["service.name"]
            == "opentelemetry-collector"
          - delete_key(attributes, "otel_scope_name") where attributes["service.name"]
            == "opentelemetry-collector"
      transform/reduce:
        error_mode: ignore
        metric_statements:
        - context: resource
          statements:
          - delete_key(attributes, "container.id")
          - delete_key(attributes, "k8s.pod.uid")
          - delete_key(attributes, "k8s.replicaset.uid")
          - delete_key(attributes, "k8s.daemonset.uid")
          - delete_key(attributes, "k8s.deployment.uid")
          - delete_key(attributes, "k8s.statefulset.uid")
          - delete_key(attributes, "k8s.cronjob.uid")
          - delete_key(attributes, "k8s.job.uid")
          - delete_key(attributes, "k8s.hpa.uid")
          - delete_key(attributes, "k8s.namespace.uid")
          - delete_key(attributes, "k8s.node.uid")
          - delete_key(attributes, "net.host.name")
          - delete_key(attributes, "net.host.port")
      transform/span_name:
        trace_statements:
        - context: span
          statements:
          - replace_pattern(name, "^(.*)$", "$$1")
      transform/spanmetrics:
        error_mode: silent
        trace_statements:
        - context: span
          statements:
          - set(attributes["http.response.status_code"], attributes["http.status_code"])
            where attributes["http.response.status_code"] == nil
      transform/test:
        error_mode: silent
        trace_statements:
        - context: span
          statements:
          - set(attributes["http.response.status_code"], attributes["http.status_code"])
            where attributes["http.response.status_code"] == nil
    receivers:
      filelog:
        exclude: []
        force_flush_period: 0
        include:
        - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        operators:
        - id: get-format
          routes:
          - expr: body matches "^\\{"
            output: parser-docker
          - expr: body matches "^[^ Z]+ "
            output: parser-crio
          - expr: body matches "^[^ Z]+Z"
            output: parser-containerd
          type: router
        - id: parser-crio
          regex: ^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: 2006-01-02T15:04:05.999999999Z07:00
            layout_type: gotime
            parse_from: attributes.time
          type: regex_parser
        - combine_field: attributes.log
          combine_with: ""
          id: crio-recombine
          is_last_entry: attributes.logtag == 'F'
          max_batch_size: 1000
          max_log_size: 1048576
          output: handle_empty_log
          source_identifier: attributes["log.file.path"]
          type: recombine
        - id: parser-containerd
          regex: ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: regex_parser
        - combine_field: attributes.log
          combine_with: ""
          id: containerd-recombine
          is_last_entry: attributes.logtag == 'F'
          max_batch_size: 1000
          max_log_size: 1048576
          output: handle_empty_log
          source_identifier: attributes["log.file.path"]
          type: recombine
        - id: parser-docker
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: json_parser
        - combine_field: attributes.log
          combine_with: ""
          id: docker-recombine
          is_last_entry: attributes.log endsWith "\n"
          max_batch_size: 1000
          max_log_size: 1048576
          output: handle_empty_log
          source_identifier: attributes["log.file.path"]
          type: recombine
        - field: attributes.log
          id: handle_empty_log
          if: attributes.log == nil
          type: add
          value: ""
        - parse_from: attributes["log.file.path"]
          regex: ^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$
          type: regex_parser
        - from: attributes.stream
          to: attributes["log.iostream"]
          type: move
        - from: attributes.container_name
          to: resource["k8s.container.name"]
          type: move
        - from: attributes.namespace
          to: resource["k8s.namespace.name"]
          type: move
        - from: attributes.pod_name
          to: resource["k8s.pod.name"]
          type: move
        - from: attributes.restart_count
          to: resource["k8s.container.restart_count"]
          type: move
        - from: attributes.uid
          to: resource["k8s.pod.uid"]
          type: move
        - default: clean-up-log-record
          routes:
          - expr: (resource["k8s.namespace.name"]) == "kube-system" && (resource["k8s.pod.name"])
              matches "app-.*" && (resource["k8s.container.name"]) == "http"
            output: kube-system_app-.*_http
          - expr: (resource["k8s.namespace.name"]) == "default" && (resource["k8s.pod.name"])
              matches "app-.*"
            output: default_app-.*
          - expr: (resource["k8s.namespace.name"]) == "test"
            output: test
          type: router
        - combine_field: attributes.log
          combine_with: ""
          id: kube-system_app-.*_http
          is_first_entry: (attributes.log) matches "^[^\\s].*"
          max_batch_size: 1000
          max_log_size: 1048576
          max_unmatched_batch_size: 1
          output: clean-up-log-record
          source_identifier: attributes["log.file.path"]
          type: recombine
        - combine_field: attributes.log
          combine_with: ""
          id: default_app-.*
          is_first_entry: (attributes.log) matches "^[^\\s].*"
          max_batch_size: 1000
          max_log_size: 1048576
          max_unmatched_batch_size: 1
          output: clean-up-log-record
          source_identifier: attributes["log.file.path"]
          type: recombine
        - combine_field: attributes.log
          combine_with: ""
          id: test
          is_first_entry: (attributes.log) matches "^[^\\s].*"
          max_batch_size: 1000
          max_log_size: 1048576
          max_unmatched_batch_size: 1
          output: clean-up-log-record
          source_identifier: attributes["log.file.path"]
          type: recombine
        - from: attributes.log
          id: clean-up-log-record
          to: body
          type: move
        - drop_ratio: 1
          expr: (attributes["log.file.path"] matches "/var/log/pods/default_example-opentelemetry-collector.*_.*/opentelemetry-collector/.*.log")
            and ((body contains "logRecord") or (body contains "ResourceLog"))
          type: filter
        - combine_field: body
          is_first_entry: body matches "^([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}|[0-9]{2}-[0-9]{2}
            [0-9]{2}:[0-9]{2}:[0-9]{2})"
          source_identifier: attributes["log.file.path"]
          type: recombine
        retry_on_failure:
          enabled: true
        start_at: beginning
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
            max_recv_msg_size_mib: 20
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 30s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      prometheus/k8s_extra_metrics:
        config:
          scrape_configs:
          - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            honor_timestamps: true
            job_name: kubernetes-cadvisor
            metrics_path: /metrics/cadvisor
            scheme: https
            static_configs:
            - targets:
              - ${env:K8S_NODE_IP}:10250
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
    service:
      extensions:
      - health_check
      - k8s_observer
      pipelines:
        logs:
          exporters:
          - debug
          - coralogix
          processors:
          - memory_limiter
          - resourcedetection/region
          - resourcedetection/env
          - k8sattributes
          - transform/k8s_attributes
          - batch
          receivers:
          - filelog
          - otlp
        metrics:
          exporters:
          - debug
          - coralogix
          processors:
          - memory_limiter
          - resourcedetection/region
          - resourcedetection/env
          - k8sattributes
          - transform/k8s_attributes
          - filter/k8s_extra_metrics
          - transform/reduce
          - transform/prometheus
          - batch
          receivers:
          - prometheus/k8s_extra_metrics
          - spanmetrics
          - spanmetrics/db
          - prometheus
          - otlp
        traces:
          exporters:
          - debug
          - spanmetrics
          - forward/db
          - forward/sampled
          processors:
          - memory_limiter
          - transform/span_name
          - resourcedetection/region
          - resourcedetection/env
          - k8sattributes
          - transform/k8s_attributes
          - transform/spanmetrics
          - transform/test
          - batch
          receivers:
          - otlp
        traces/db:
          exporters:
          - spanmetrics/db
          processors:
          - filter/db_spanmetrics
          - transform/db
          - batch
          receivers:
          - forward/db
        traces/sampled:
          exporters:
          - coralogix
          processors:
          - batch
          - probabilistic_sampler
          receivers:
          - forward/sampled
      telemetry:
        logs:
          encoding: json
        metrics:
          readers:
          - pull:
              exporter:
                prometheus:
                  host: ${env:MY_POD_IP}
                  port: 8888
